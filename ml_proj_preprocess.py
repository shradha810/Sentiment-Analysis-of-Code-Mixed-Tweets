# -*- coding: utf-8 -*-
"""ML Proj - Preprocess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GeUnVZaRHB-0dO0R4KxzNfwXtrkX0BMM
"""

from google.colab import drive
drive.mount('/content/drive')

pip install googletrans

pip install autocorrect

!cd ./py-googletrans && python setup.py install

import pandas as pd
import numpy as np
import joblib
from autocorrect import Speller
from googletrans import Translator
import re
import nltk
nltk.download('wordnet')
from nltk.corpus import wordnet
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
nltk.download('punkt')
from nltk.stem import WordNetLemmatizer 
from sklearn.preprocessing import LabelEncoder
nltk.download('averaged_perceptron_tagger') 
nltk.download('sentiwordnet')
from nltk.corpus import sentiwordnet as swn
from sklearn.metrics import classification_report
from gensim.models import Word2Vec
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import tensorflow as tf
import joblib
import keras
from tensorflow.keras import layers, models
from keras.optimizers import SGD
import matplotlib.pyplot as plt
from keras.layers import BatchNormalization,Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

"""Total number of tweets: 6137
      positive: 2063
      negative: 1752
      neutral: 2322
"""

def make_list(train_file,label_file):
  # train_word_list: list of list of word wise individual tweets
  # train_lang_list: list of list of word wise language of individual tweets
  # train_sentiment: list of sentence wise sentiment of individual tweets
  train_word_list=[]
  train_lang_list=[]
  train_sentiment=[]
  if label_file == 'train':
    for i in range(len(train_file[0])):
      if train_file[0][i]=="meta":
        word_list=''
        lang_list=[]
        train_sentiment.append(train_file[2][i])
        for j in range(i+1,len(train_file[0])):
          if train_file[0][j]=="meta":
            i = j-1
            break
          else:
            word_list = word_list + ' '+ str(train_file[0][j])
            lang_list.append(train_file[1][j])
        train_word_list.append(word_list)
        train_lang_list.append(lang_list)
  else:
    with open(label_file) as f:
      content = f.readlines()[1:]
    content = [x.strip() for x in content] 
    sent_list=[]
    for data in content:
      sentiment=re.findall(r',(\w+)', data)
      sent_list.append(sentiment)
    sent_list=np.array(sent_list)
    train_sentiment=sent_list.flatten()

    for i in range(len(train_file[0])):
      if train_file[0][i]=="meta":
        word_list=''
        lang_list=[]
        for j in range(i+1,len(train_file[0])):
          if train_file[0][j]=="meta":
            i = j-1
            break
          else:
            word_list = word_list + ' '+ str(train_file[0][j])
            lang_list.append(train_file[1][j])
        train_word_list.append(word_list)
        train_lang_list.append(lang_list)
  return train_word_list, train_lang_list, train_sentiment

# tranlate hindi text to english
def translate(train_word_list):
  translator = Translator()
  for i in range(len(train_word_list)):
    result = translator.translate(train_word_list[i],src='hi')
    train_word_list[i]=result.text
  print(train_word_list[1])
  return train_word_list

# remove rsymbols like , . #
def remove_symbols(train_word_list):
  symbols=(',','.', '!', '/','#','@',';','_','-','~','(',')','[',']')
  no_sym_list=[]
  for i in range(len(train_word_list)):
    word=''
    for j in train_word_list[i]:
      if j not in symbols:
        word = word +j
    no_sym_list.append(word)
  return no_sym_list

# correct the spellings
def spellcheck(train_word_list):
  spell = Speller(lang='en')
  for i in range(len(train_word_list)):
    result = spell(train_word_list[i])
    train_word_list[i]=result.lower()
  print(train_word_list[1])
  return train_word_list

# replace words following not with their antonym
def replace_antonym(train_word_list):
  list_train_word = []
  for i in range(len(train_word_list)):
    train_word = train_word_list[i].split()
    list_train_word.append(train_word)
  for i in range(len(list_train_word)):
    for j in range(len(list_train_word[i])-1):
      flag=1
      if list_train_word[i][j]=='not':
        for synset in wordnet.synsets(list_train_word[i][j+1]):
          for lemma in synset.lemmas():
            if lemma.antonyms():
              list_train_word[i][j+1]=lemma.antonyms()[0].name()
              flag=0
              break
          if flag==0:
            break
  list1_train_word=[]
  for i in list_train_word:
    sentence=''
    for word in i:
      sentence = sentence + ' '+word
    list1_train_word.append(sentence)
  return list1_train_word

# remove stop words
def stopword_removal(list_train_word):
  list1_train_word=[]
  for i in list_train_word:
    list1_train_word.append(i.split())
  new_train_list=[]
  for i in range(len(list1_train_word)):
    new_sentence=[]
    for word in list1_train_word[i]:
      if word not in stopwords.words():
        new_sentence.append(word)
    new_train_list.append(' '.join(new_sentence))
  return new_train_list

# lemmatise
def lemmatisation(list_train_word):
  lemmatiser = WordNetLemmatizer()
  new_lis=[]
  for i in list1_train_word:
    sentence = nltk.word_tokenize(i)
    pos = nltk.pos_tag(sentence)
    lem = []
    for word, pos_tag in pos:
      if pos_tag[0]=='J':
        pos_tag= wordnet.ADJ
      elif pos_tag[0]=='V':
        pos_tag=wordnet.VERB
      elif pos_tag[0]=='R':
        pos_tag=wordnet.ADV
      elif pos_tag[0]=='N':
        pos_tag=wordnet.NOUN
      else:
        pos_tag= None 
      if pos_tag is not None:
        lem.append(lemmatiser.lemmatize(word,pos_tag))
      else:
        lem.append(lemmatiser.lemmatize(word))
    new_lis.append(' '.join(lem))
  return new_lis

"""Preprocessing"""

def preprocess(train_file,label_file):
  # removing tweeter handles and O and EMT language data
  train_file=train_file[~(train_file[0].shift(1)== "@")]
  train_file=train_file[~ (train_file[1]=="O")]
  train_file=train_file[~ (train_file[1]=="EMT")]
  train_file = train_file.reset_index(drop=True)
  train_word_list, train_lang_list, train_sentiment=make_list(train_file,label_file)
  train_word_list=translate(train_word_list)
  train_word_list=remove_symbols(train_word_list)
  train_word_list=spellcheck(train_word_list)
  list_train_word = replace_antonym(train_word_list)
  #list_train_word=lemmatisation(list_train_word)
  list_train_word = stopword_removal(list_train_word)
  train_sentiment = LabelEncoder().fit_transform(train_sentiment)
  return list_train_word, train_sentiment

"""Feature Extraction"""

def pos_to_WordNet(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    elif tag.startswith('V'):
        return wordnet.VERB
    return None

def pos_tagChange(tokens_pos_tags):
  tokens_wn_tags = []
  for w_tags in tokens_pos_tags:
    tag = pos_to_WordNet(w_tags[1])
    #print(tag)
    if tag is not None:
      tokens_wn_tags.append((w_tags[0], tag))
  return tokens_wn_tags

def sentiWord(tokens_pos_tags):
  featureSet = []
  for word_tag in tokens_pos_tags:
    score = list(swn.senti_synsets(word_tag[0], pos=word_tag[1]))
    if len(score) != 0:
      featureSet.append((score[0].pos_score(), score[0].neg_score(), score[0].obj_score()))
  return np.array(featureSet)

def sentiWordFeatures(list_train_word):
  list_train_sentences = []
  for sentence in list_train_word:
    tokens = nltk.word_tokenize(sentence)
    tokens_pos_tags = nltk.pos_tag(tokens)
    tokens_wn_tags = pos_tagChange(tokens_pos_tags)
    if len(tokens_pos_tags) != 0:
      sentiFeatures = sentiWord(tokens_wn_tags)
      list_train_sentences.append(sentiFeatures)
    else:
      list_train_sentences.append([])
  return np.array(list_train_sentences)

def tokenizeSentences(list_train_word):
  list_tokenized_sentences = []
  for sentence in list_train_word:
    tokens = nltk.word_tokenize(sentence)
    list_tokenized_sentences.append(tokens)
  return np.array(list_tokenized_sentences)

w2v_size = 128
def word2VecFeatures(list_train_word, flag):
  tokenizedSentences = tokenizeSentences(list_train_word)
  if flag==1:
    model = Word2Vec(tokenizedSentences, min_count=5, size=w2v_size, window=10, sg=1, iter=500, sample=1e-3, workers=10)
    joblib.dump(model,'/content/drive/My Drive/MLproject/word2vec_train.pkl')
  elif flag==3:
    model = joblib.load('/content/drive/My Drive/MLproject/word2vec_train.pkl')
  featureSet2 = []
  for sentence in tokenizedSentences:
    feature = []
    for word in sentence:
      if word in model.wv.vocab:
        feature.append(model[word])
    featureSet2.append(np.array(feature))
  return np.array(featureSet2)

def featureExtraction(list_sentences,flag):
  featureSet1 = sentiWordFeatures(list_sentences)
  featureSet2 = word2VecFeatures(list_sentences,flag)
  #X_feature = np.c_[np.array(featureSet1, dtype=object), np.array(featureSet2, dtype=object)]
  X_feature1 = [[], [], []]
  for feature in featureSet1:
    sentiFeature = [[], [], []]
    for feat in feature:
      sentiFeature[0].append(feat[0])
      sentiFeature[1].append(feat[1])
      sentiFeature[2].append(feat[2])
    X_feature1[0].append(np.mean(sentiFeature[0]))
    X_feature1[1].append(np.mean(sentiFeature[1]))
    X_feature1[2].append(np.mean(sentiFeature[2]))

  X_feature1 = np.array(X_feature1).reshape(len(X_feature1[0]), 3)

  X_feature2 = np.zeros(shape=(len(featureSet2), w2v_size))
  k = 0
  for feature in featureSet2:
    w2vFeature = [[] for i in range(w2v_size)]
    for feat in feature:
      #print(feat)
      for i in range(len(feat)):
        w2vFeature[i].append(feat[i])
    #w2vMean = [] * featureSet2.shape[1]
    for i in range(len(feat)):
      #w2vMean[i].append(np.mean(w2vFeature[:,i]))
      X_feature2[k][i] = np.mean(w2vFeature[i])
    k += 1

  #X_feature2 = np.array(X_feature2)

  X_feature1 = np.nan_to_num(X_feature1)
  X_feature2 = np.nan_to_num(X_feature2)

  return np.c_[X_feature1, X_feature2]

"""##Logistic regression

**Train** accuracy - 57.8%; 
Validation accuracy - 52.3%; 
Test accuracy - 57.2%, F1 score - 57%
"""

def model_logistic_reg():
  #train_file = pd.read_csv('/content/drive/My Drive/MLproject/train_14k_split_conll.txt', sep='\t', header=None)
  #train_file.head(30)
  #list_train_word, train_sentiment=preprocess(train_file, 'train')
  #joblib.dump(list_train_word,'/content/drive/My Drive/MLproject/X_Train.pkl')
  #joblib.dump(train_sentiment,'/content/drive/My Drive/MLproject/Y_Train.pkl')
  list_train_word = joblib.load('/content/drive/My Drive/MLproject/X_Train.pkl')
  train_sentiment=joblib.load('/content/drive/My Drive/MLproject/Y_Train.pkl')
  X_train_feature = featureExtraction(list_train_word,1)
  model = LogisticRegression(max_iter=500).fit(X_train_feature, train_sentiment)
  joblib.dump(model,'/content/drive/My Drive/MLproject/LogReg.pkl')
  model = joblib.load('/content/drive/My Drive/MLproject/LogReg.pkl')
  y_pred = model.predict(X_train_feature)
  train_accuracy = accuracy_score(train_sentiment,y_pred)
  print("train accuracy: ",train_accuracy*100)

  #valid_file = pd.read_csv('/content/drive/My Drive/MLproject/dev_3k_split_conll.txt', sep='\t', header=None)
  #train_file.head(30)
  #x_valid, y_valid=preprocess(valid_file,'train')
  #joblib.dump(x_valid,'/content/drive/My Drive/MLproject/x_valid.pkl')
  #joblib.dump(y_valid,'/content/drive/My Drive/MLproject/y_valid.pkl')
  x_valid=joblib.load('/content/drive/My Drive/MLproject/x_valid.pkl')
  y_valid=joblib.load('/content/drive/My Drive/MLproject/y_valid.pkl')
  x_valid_feature = featureExtraction(x_valid,1)
  y_valid_pred = model.predict(x_valid_feature)
  valid_accuracy = accuracy_score(y_valid,y_valid_pred)
  print("validation accuracy: ",valid_accuracy*100)

  #test_file = pd.read_csv('/content/drive/My Drive/MLproject/Hindi_test_unalbelled_conll_updated.txt', sep='\t', header=None)
  #train_file.head(30)
  #x_test, y_test=preprocess(test_file,'/content/drive/My Drive/MLproject/test_labels_hinglish.txt')
  #joblib.dump(x_test,'/content/drive/My Drive/MLproject/x_test.pkl')
  #joblib.dump(y_test,'/content/drive/My Drive/MLproject/y_test.pkl')
  x_test=joblib.load('/content/drive/My Drive/MLproject/x_test.pkl')
  y_test=joblib.load('/content/drive/My Drive/MLproject/y_test.pkl')
  x_test_feature = featureExtraction(x_test,1)
  y_test_pred = model.predict(x_test_feature)
  test_accuracy = accuracy_score(y_test,y_test_pred)
  print("test accuracy: ",test_accuracy*100)
  print(classification_report(y_test, y_test_pred))

# model_logistic_reg()

"""#Random Forest

**Train** accuracy - 62.8%; 
Validation accuracy - 54.0%; 
Test accuracy - 58.2%
"""

from sklearn.ensemble import RandomForestClassifier
def model_random_forest():
  list_train_word = joblib.load('/content/drive/My Drive/MLproject/X_Train.pkl')
  train_sentiment=joblib.load('/content/drive/My Drive/MLproject/Y_Train.pkl')
  X_train_feature = featureExtraction(list_train_word,1)
  model = RandomForestClassifier(n_estimators=500, min_samples_leaf=50)
  model.fit(X_train_feature,train_sentiment)
  joblib.dump(model,'/content/drive/My Drive/MLproject/RandForest.pkl')
  model = joblib.load('/content/drive/My Drive/MLproject/RandForest.pkl')
  y_pred = model.predict(X_train_feature)
  train_accuracy = accuracy_score(train_sentiment,y_pred)
  print("train accuracy: ",train_accuracy*100)

  x_valid=joblib.load('/content/drive/My Drive/MLproject/x_valid.pkl')
  y_valid=joblib.load('/content/drive/My Drive/MLproject/y_valid.pkl')
  x_valid_feature = featureExtraction(x_valid,1)
  y_valid_pred = model.predict(x_valid_feature)
  valid_accuracy = accuracy_score(y_valid,y_valid_pred)
  print("validation accuracy: ",valid_accuracy*100)

  x_test=joblib.load('/content/drive/My Drive/MLproject/x_test.pkl')
  y_test=joblib.load('/content/drive/My Drive/MLproject/y_test.pkl')
  x_test_feature = featureExtraction(x_test,1)
  y_test_pred = model.predict(x_test_feature)
  test_accuracy = accuracy_score(y_test,y_test_pred)
  print("test accuracy: ",test_accuracy*100)
  print(classification_report(y_test, y_test_pred))
#model_random_forest()

"""##SVM

**Train** accuracy - 65.8%; 
Validation accuracy - 55.65%; 
Test accuracy - 59.32%
"""

def model_svc():
  list_train_word = joblib.load('/content/drive/My Drive/MLproject/X_Train.pkl')
  train_sentiment=joblib.load('/content/drive/My Drive/MLproject/Y_Train.pkl')
  X_train_feature = featureExtraction(list_train_word,1)
  model = SVC(C=1.0, kernel='rbf')
  model.fit(X_train_feature,train_sentiment)
  joblib.dump(model,'/content/drive/My Drive/MLproject/SVC_model.pkl')
  model = joblib.load('/content/drive/My Drive/MLproject/SVC_model.pkl')
  y_pred = model.predict(X_train_feature)
  train_accuracy = accuracy_score(train_sentiment,y_pred)
  print("train accuracy: ",train_accuracy*100)

  x_valid=joblib.load('/content/drive/My Drive/MLproject/x_valid.pkl')
  y_valid=joblib.load('/content/drive/My Drive/MLproject/y_valid.pkl')
  x_valid_feature = featureExtraction(x_valid,1)
  y_valid_pred = model.predict(x_valid_feature)
  valid_accuracy = accuracy_score(y_valid,y_valid_pred)
  print("validation accuracy: ",valid_accuracy*100)

  x_test=joblib.load('/content/drive/My Drive/MLproject/x_test.pkl')
  y_test=joblib.load('/content/drive/My Drive/MLproject/y_test.pkl')
  x_test_feature = featureExtraction(x_test,1)
  y_test_pred = model.predict(x_test_feature)
  test_accuracy = accuracy_score(y_test,y_test_pred)
  print("test accuracy: ",test_accuracy*100)
  print(classification_report(y_test, y_test_pred))

#model_svc()

'''list_train_word = joblib.load('/content/drive/My Drive/MLproject/X_Train.pkl')
train_sentiment=joblib.load('/content/drive/My Drive/MLproject/Y_Train.pkl')
X_train_feature = featureExtraction(list_train_word,1)

parameters = {'kernel':('linear', 'rbf'), 'C':[0.1, 0.01, 1, 10, 100]}
svc = SVC()
clf = GridSearchCV(svc, parameters)
clf.fit(X_train_feature, train_sentiment)
clf.best_params_'''

def word_embeddings():
  embeddings_dict = {}
  with open("/content/drive/MyDrive/MLproject/glove.6B.50d.txt", 'r') as f:
      for line in f:
          values = line.split()
          word = values[0]
          vector = np.asarray(values[1:], "float32")
          embeddings_dict[word] = vector
  return embeddings_dict

'''embeddings_dict = word_embeddings()
print(embeddings_dict)

x_train = joblib.load('/content/drive/My Drive/MLproject/X_Train.pkl')
y_train=joblib.load('/content/drive/My Drive/MLproject/Y_Train.pkl')
x_test=joblib.load('/content/drive/My Drive/MLproject/x_test.pkl')
y_test=joblib.load('/content/drive/My Drive/MLproject/y_test.pkl')

'''
def max_sequence_length(texts):
  MAX_SEQUENCE_LENGTH = 0
  for x in texts:
    length = len(x.split())
    if length > MAX_SEQUENCE_LENGTH:
      MAX_SEQUENCE_LENGTH = length

def tokenize_texts():
  tokenizer = Tokenizer(num_words=MAX_SEQUENCE_LENGTH)
  tokenizer.fit_on_texts(x_train)
  sequences = tokenizer.texts_to_sequences(x_train)

  word_index_train = tokenizer.word_index
  data_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)

  #tokenizer = Tokenizer(num_words=MAX_SEQUENCE_LENGTH)
  #tokenizer.fit_on_texts(x_test)
  sequences = tokenizer.texts_to_sequences(x_test)
  #print('Found %s unique tokens.' % len(word_index))

  data_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)

  return data_train, data_test, word_index_train

EMBEDDING_DIM = 50
def embedding_matrix():
  embedding_matrix = np.zeros((len(word_index_train) + 1, EMBEDDING_DIM))
  for word, i in word_index_train.items():
      embedding_vector = embeddings_dict.get(word)
      if embedding_vector is not None:
          # words not found in embedding index will be all-zeros.
          embedding_matrix[i] = embedding_vector
  return embedding_matrix

'''MAX_SEQUENCE_LENGTH = max_sequence_length(x_train)
data_train, data_test, word_index_train = tokenize_texts()
embedding_matrix = embedding_matrix()'''

def model_rnn_word():
  model = keras.Sequential()
  model.add(layers.Embedding(len(word_index_train) + 1,
                              EMBEDDING_DIM,
                              weights=[embedding_matrix],
                              input_length=MAX_SEQUENCE_LENGTH,
                              trainable=False))

  model.add(layers.SimpleRNN(128))

  # Add a Dense layer with 3 units.
  model.add(layers.Dense(3, activation='softmax'))

  model.summary()

  y_train_d = keras.utils.to_categorical(y_train,3)
  y_test_d = keras.utils.to_categorical(y_test,3)
  model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.1), metrics=["accuracy"])
  history = model.fit(data_train, y_train_d, epochs=100)
  loss,accuracy = model.evaluate(x=data_test, y=y_test_d)
#model_rnn_word()

"""##Simple RNN

**Train** accuracy - 70.8%; 
Validation accuracy - 65.3%; 
Test accuracy - 62.5%, F1 score - 62.7%
"""

def model_rnn():
  list_train_word = joblib.load('/content/drive/My Drive/MLproject/X_Train.pkl')
  y_train=joblib.load('/content/drive/My Drive/MLproject/Y_Train.pkl')
  x_test=joblib.load('/content/drive/My Drive/MLproject/x_test.pkl')
  y_test=joblib.load('/content/drive/My Drive/MLproject/y_test.pkl')
  X_train_feature = featureExtraction(list_train_word,1)
  X_test_feature = featureExtraction(x_test,1)

  X_train_feature = X_train_feature.reshape(X_train_feature.shape[0], 1, X_train_feature.shape[1])
  X_test_feature = X_test_feature.reshape(X_test_feature.shape[0], 1, X_test_feature.shape[1])

  model = keras.Sequential()

  model.add(layers.SimpleRNN(128, input_shape = (1, 131)))
  model.add(layers.Dropout(0.3))
  # Add a Dense layer with 3 units.
  model.add(layers.Dense(3, activation='softmax'))

  y_train_d = keras.utils.to_categorical(y_train,3)
  y_test_d = keras.utils.to_categorical(y_test,3)
  model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001), metrics=["accuracy"])
  history = model.fit(X_train_feature, y_train_d, epochs=100)
  model.summary()
  loss,accuracy = model.evaluate(x=X_test_feature, y=y_test_d)
#model_rnn()

def model_rnn1():
  list_train_word = joblib.load('/content/drive/My Drive/MLproject/X_Train.pkl')
  y_train=joblib.load('/content/drive/My Drive/MLproject/Y_Train.pkl')
  x_test=joblib.load('/content/drive/My Drive/MLproject/x_test.pkl')
  y_test=joblib.load('/content/drive/My Drive/MLproject/y_test.pkl')
  x_valid=joblib.load('/content/drive/My Drive/MLproject/x_valid.pkl')
  y_valid=joblib.load('/content/drive/My Drive/MLproject/y_valid.pkl')
  
  X_valid_feature = featureExtraction(x_valid,1)
  X_train_feature = featureExtraction(list_train_word,1)
  X_test_feature = featureExtraction(x_test,1)

  X_train_feature = X_train_feature.reshape(X_train_feature.shape[0], 1, X_train_feature.shape[1])
  X_valid_feature = X_valid_feature.reshape(X_valid_feature.shape[0], 1, X_valid_feature.shape[1])
  X_test_feature = X_test_feature.reshape(X_test_feature.shape[0], 1, X_test_feature.shape[1])

  model = keras.Sequential()

  model.add(layers.SimpleRNN(128, input_shape = (1, 131)))
  model.add(layers.BatchNormalization())
  model.add(layers.Dropout(0.4))
  model.add(layers.Dense(3, activation='softmax'))

  y_train_d = keras.utils.to_categorical(y_train,3)
  y_valid_d = keras.utils.to_categorical(y_valid,3)
  y_test_d = keras.utils.to_categorical(y_test,3)
  model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.0001), metrics=["accuracy"])
  
  history = model.fit(X_train_feature, y_train_d, validation_data = (X_valid_feature, y_valid_d), batch_size=30,epochs=300)
  model.summary()
  loss,accuracy = model.evaluate(x=X_test_feature, y=y_test_d)
  y_test_pred=model.predict(X_test_feature)
  print(classification_report(y_test_d.argmax(axis=1), y_test_pred.argmax(axis=1)))
#model_rnn1()